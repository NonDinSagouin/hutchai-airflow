import sys
import os

from datetime import datetime, timedelta

from airflow import DAG

sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

import app.tasks.z_exemple as z_exemple

DAG_ID = "z_Stress_test"
DESCRIPTION = "R√©alisation de tests de stress sur une SparkSession Airflow."
OBJECTIF = "Ce DAG a pour objectif de r√©aliser des tests de stress sur une SparkSession Airflow en √©valuant les performances en termes de m√©moire et de CPU."

default_args = {
    'owner': 'airflow',
    'depends_on_past': False, # Attendre l'ex√©cution pr√©c√©dente
    'retries': 0, # Nombre de tentatives avant l'√©chec d'une t√¢che
}

# D√©finition du DAG
with DAG(
    dag_id=DAG_ID, # Identifiant unique du DAG
    default_args=default_args, # Dictionnaire contenant les param√®tres par d√©faut des t√¢ches
    start_date=datetime(2025, 1, 1), # Date de d√©but du DAG
    tags=["exemple",], # Liste de tags pour cat√©goriser le DAG dans l'UI
    catchup=False, # Ex√©cution des t√¢ches manqu√©es (True ou False)
    max_active_runs=1,  # Limite √† 1 ex√©cutions actives en m√™me temps
    dagrun_timeout=timedelta(minutes=5),  # Timeout pour chaque ex√©cution du DAG
    description=DESCRIPTION,
    doc_md=f"""
        ## üîπ Description
        {DESCRIPTION}

        ## üîπ Objectif
        {OBJECTIF}
    """,
) as dag:

    task_spark_stress_test_memory = z_exemple.Stress_test.spark_stress_test_memory(
        task_id="task_spark_stress_test_memory",
    )

    task_spark_stress_test_cpu = z_exemple.Stress_test.spark_stress_test_cpu(
        task_id="task_spark_stress_test_cpu",
    )

    [task_spark_stress_test_memory, task_spark_stress_test_cpu]