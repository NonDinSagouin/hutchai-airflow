import sys
import os
import logging

from datetime import datetime, timedelta

from airflow import DAG
from airflow.sdk import chain

sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

import app.tasks.api as api
import app.tasks.databases as databases
import app.tasks.transformation as transformation
import app.manager as manager

from app.tasks.decorateurs import customTask

DAG_ID = "LOL_kpis_stats"
DESCRIPTION = ""
OBJECTIF = ""
SCHEDULE = None
START_DATE = datetime(2025, 1, 1)
TAGS = []

default_args = {
    'owner': 'airflow',
    'depends_on_past': False, # Attendre l'ex√©cution pr√©c√©dente
    'retries': 0, # Nombre de tentatives avant l'√©chec d'une t√¢che
    'retry_delay': timedelta(seconds=10), # Temps entre chaque tentative
}

class Custom():

    @customTask
    @staticmethod
    def vide(**kwargs) -> None:
        """ Description de la fonction vide.
        Returns:
            None
        """
        spark = manager.Spark.get(
            app_name="LOL_KPI_DamagePerMinute",
            driver_memory="2g",
            sql_shuffle_partitions="4",
            **kwargs
        )

        logging.info("üî• D√©but du calcul des KPI de d√©g√¢ts par minute")

        postgres_engine = manager.Connectors.postgres("POSTGRES_warehouse")
        jdbc_url = postgres_engine.url.render_as_string(hide_password=False).replace("postgresql+psycopg2://", "jdbc:postgresql://")

        # Configuration JDBC pour PostgreSQL
        jdbc_properties = {
            "driver": "org.postgresql.Driver",
            "user": postgres_engine.url.username,
            "password": postgres_engine.url.password,
        }

        # Charger les donn√©es depuis PostgreSQL
        logging.info("üìä Chargement des donn√©es depuis PostgreSQL...")

        # Lire la table des PUUIDs √† traiter
        df_puuid = spark.read.jdbc(
            url=jdbc_url,
            table="lol_fact_datas.lol_fact_puuid_to_process",
            properties=jdbc_properties
        )
        pandas_df = df_puuid.toPandas()
        print(pandas_df)


# D√©finition du DAG
with DAG(
    dag_id=DAG_ID, # Identifiant unique du DAG
    default_args=default_args, # Dictionnaire contenant les param√®tres par d√©faut des t√¢ches
    start_date=START_DATE, # Date de d√©but du DAG
    #schedule=SCHEDULE,  # Fr√©quence d'ex√©cution (CRON ou timedelta)
    tags=TAGS, # Liste de tags pour cat√©goriser le DAG dans l'UI
    catchup=False, # Ex√©cution des t√¢ches manqu√©es (True ou False)
    max_active_runs=1,  # Limite √† 1 ex√©cutions actives en m√™me temps
    dagrun_timeout=timedelta(minutes=15),
    description=DESCRIPTION,
    doc_md=f"""
        ## üîπ Description
        {DESCRIPTION}

        ## üîπ Objectif
        {OBJECTIF}
    """,
) as dag:

    task_vide = Custom.vide(
        task_id = "task_vide",
    )

    chain(
        task_vide
    )
