x-spark-common: &spark-common
  build:
      context: .
      dockerfile: build/spark.dockerfile
  environment:
    SPARK_NO_DAEMONIZE: "true"
    PYSPARK_PYTHON: "python3"
  networks:
    - spark-net
    - airflow_default
  restart: always

x-worker-common: &worker-common
  << : *spark-common
  command: >
    /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker
    spark://spark-master:7077
    --cores 2
    --memory 3G
  mem_limit: 4g
  cpus: 2
  depends_on:
    - spark-master

services:
  spark-master:
    << : *spark-common
    container_name: spark-master
    command: >
      /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
      --host spark-master
    ports:
      - "8090:8080"
      - "7077:7077"
    mem_limit: 2g
    cpus: 1

  spark-worker-1:
    << : *worker-common
    container_name: spark-worker-1

  spark-worker-2:
    << : *worker-common
    container_name: spark-worker-2