import logging
import socket

from pyspark.sql import SparkSession
from airflow.sdk.bases.hook import BaseHook
from airflow.exceptions import AirflowFailException

import app.helper as helper

class Spark:
    """Gestionnaire de SparkSession pour Airflow."""

    __session: dict[str, SparkSession] = {}

    @staticmethod
    def get(
        conn_id: str = "spark_default",
        app_name: str = "default_task_id",
        driver_memory: str = "1g",
        driver_max_result_size: str = "1g",
        driver_bind_address: str = "0.0.0.0",
        sql_adaptive_enabled: str = "true",
        sql_adaptive_coalesce_partitions_enabled: str = "true",
        serializer: str = "org.apache.spark.serializer.KryoSerializer",
        sql_shuffle_partitions: str = "8",
        executor_heartbeat_interval: str = "60s",
        network_timeout: str = "300s",
        spark_configs: dict = None,
        **kwargs
    ) -> SparkSession:
        """Retourne la SparkSession existante ou en cr√©e une nouvelle si n√©cessaire.

        Args:
            conn_id: ID de la connexion Airflow Spark
            app_name: Nom de l'application Spark (par d√©faut: task_id)
            driver_memory: M√©moire allou√©e au driver
            driver_max_result_size: Taille maximale des r√©sultats du driver
            driver_bind_address: Adresse de bind du driver
            sql_adaptive_enabled: Active l'ex√©cution adaptative SQL
            sql_adaptive_coalesce_partitions_enabled: Active la coalescence des partitions
            serializer: S√©rialiseur Spark √† utiliser
            sql_shuffle_partitions: Nombre de partitions pour les shuffles
            executor_heartbeat_interval: Intervalle de heartbeat des executors
            network_timeout: Timeout r√©seau
            spark_configs: Dictionnaire de configurations Spark suppl√©mentaires
            **kwargs: Arguments additionnels (task_id, etc.)

        Returns:
            SparkSession configur√©e
        """

        helper.logging_title("‚è≥ Configuration de la SparkSession", lvl=3)

        task_id = kwargs.get("task_id", "default_task_id")
        app_name = app_name or task_id

        if app_name not in Spark.__session and not Spark.__set(
            conn_id=conn_id,
            app_name=app_name,
            driver_memory=driver_memory,
            driver_max_result_size=driver_max_result_size,
            driver_bind_address=driver_bind_address,
            sql_adaptive_enabled=sql_adaptive_enabled,
            sql_adaptive_coalesce_partitions_enabled=sql_adaptive_coalesce_partitions_enabled,
            serializer=serializer,
            sql_shuffle_partitions=sql_shuffle_partitions,
            executor_heartbeat_interval=executor_heartbeat_interval,
            network_timeout=network_timeout,
            spark_configs=spark_configs,
        ):
            raise AirflowFailException("Impossible de cr√©er une SparkSession Spark.")

        helper.logging_title("‚úÖ SparkSession obtenue.", lvl=3, close=True)

        return Spark.__session[app_name]

    @staticmethod
    def close(
        app_name: str = "default_task_id",
        **kwargs
    ) -> None:
        """Ferme la SparkSession proprement.

        Args:
            app_name: Nom de l'application Spark (par d√©faut: task_id)
            **kwargs: Arguments additionnels (task_id, etc.)
        """

        task_id = kwargs.get("task_id", "default_task_id")
        app_name = app_name or task_id

        if app_name not in Spark.__session:
            logging.warning("‚ö†Ô∏è Aucune SparkSession active √† fermer.")
            return

        try:
            logging.info("‚è≥ Fermeture de la SparkSession...")
            Spark.__session[app_name].stop()
            logging.info("‚úÖ SparkSession ferm√©e.")

            del Spark.__session[app_name]
            logging.info("‚úÖ SparkSession r√©initialis√©e.")

        except Exception as e:
            raise AirflowFailException("‚ùå √âchec de la fermeture de la SparkSession.") from e

        finally:
            if app_name in Spark.__session:
                del Spark.__session[app_name]  # R√©initialiser quand m√™me
            return

    @staticmethod
    def __set(
        conn_id: str = "spark_default",
        app_name: str = "default_task_id",
        driver_memory: str = "1g",
        driver_max_result_size: str = "1g",
        driver_bind_address: str = "0.0.0.0",
        sql_adaptive_enabled: str = "true",
        sql_adaptive_coalesce_partitions_enabled: str = "true",
        serializer: str = "org.apache.spark.serializer.KryoSerializer",
        sql_shuffle_partitions: str = "8",
        executor_heartbeat_interval: str = "60s",
        network_timeout: str = "300s",
        spark_configs: dict = None
    ) -> bool:
        """Configure et retourne une SparkSession √† partir d'une connexion Airflow."""

        if app_name in Spark.__session:
            logging.warning("‚ö†Ô∏è Une SparkSession existe d√©j√†. Utilisation de la session existante.")
            return False

        try:
            # R√©cup√©rer la connexion Spark depuis Airflow
            conn = BaseHook.get_connection(conn_id)
            host = conn.host
            port = conn.port

            # R√©cup√©rer les configurations depuis extra (JSON)
            extra = conn.extra_dejson
            executor_memory = extra.get("executor-memory", "1g")
            total_executor_cores = extra.get("total-executor-cores", "2")
            executor_cores = extra.get("executor-cores", "2")

            # Obtenir l'adresse IP du conteneur
            hostname = socket.gethostname()
            host_ip = socket.gethostbyname(hostname)

            logging.info(f"‚ÑπÔ∏è Driver hostname: {hostname}, IP: {host_ip}")
            logging.info(f"‚ÑπÔ∏è Connecting to Spark cluster at spark://{host}:{port}")

            # Cr√©er la session Spark avec configuration optimis√©e
            builder = SparkSession.builder \
                .appName(app_name) \
                .master(f"spark://{host}:{port}") \
                .config("spark.driver.host", host_ip) \
                .config("spark.driver.bindAddress", driver_bind_address) \
                .config("spark.executor.memory", executor_memory) \
                .config("spark.executor.cores", executor_cores) \
                .config("spark.cores.max", total_executor_cores) \
                .config("spark.driver.memory", driver_memory) \
                .config("spark.driver.maxResultSize", driver_max_result_size) \
                .config("spark.sql.adaptive.enabled", sql_adaptive_enabled) \
                .config("spark.sql.adaptive.coalescePartitions.enabled", sql_adaptive_coalesce_partitions_enabled) \
                .config("spark.serializer", serializer) \
                .config("spark.default.parallelism", str(int(total_executor_cores) * 2)) \
                .config("spark.sql.shuffle.partitions", sql_shuffle_partitions) \
                .config("spark.executor.heartbeatInterval", executor_heartbeat_interval) \
                .config("spark.network.timeout", network_timeout)

            # Ajouter les configurations suppl√©mentaires si fournies
            if spark_configs:
                for key, value in spark_configs.items():
                    builder = builder.config(key, value)

            Spark.__session[app_name] = builder.getOrCreate()

            # V√©rifier que les executors sont bien connect√©s
            sc = Spark.__session[app_name].sparkContext
            logging.info("üîµ V√âRIFICATION DE LA CONNEXION SPARK üîµ")
            logging.info(f"üîπApplication ID: {sc.applicationId}")
            logging.info(f"üîπMaster URL: {sc.master}")
            logging.info(f"üîπDefault parallelism: {sc.defaultParallelism}")

            # Log de la configuration finale
            logging.info("üîµ CONFIGURATION SPARK ACTIVE üîµ")
            logging.info(f"üîπExecutors configur√©s: {total_executor_cores} cores total")
            logging.info(f"üîπM√©moire par executor: {executor_memory}")
            logging.info(f"üîπParallelism: {sc.defaultParallelism}")

            return True

        except Exception as e:
            raise AirflowFailException(f"‚ùå √âchec de connexion au cluster Spark {conn_id} ! {e}")
