import pandas as pd
import logging
import re
import pendulum

from typing import Any
from datetime import datetime, timezone

from sqlalchemy.engine import Engine
from sqlalchemy import text
from airflow.exceptions import AirflowFailException, AirflowSkipException

import app.helper as helper
import app.manager as manager

from app.tasks.decorateurs import customTask

class PostgresWarehouse():

    @staticmethod
    def __setup_schema(engine: Engine, schema: str):
        """ Cr√©e un sch√©ma dans la base de donn√©es s'il n'existe pas d√©j√†."""

        if not schema or not schema.strip():
            raise ValueError("Le nom du sch√©ma ne peut pas √™tre vide")

        if not re.match(r'^[a-zA-Z0-9_]+$', schema):
            raise ValueError(f"Nom de sch√©ma invalide: '{schema}'")

        logging.info(f"‚è≥ V√©rification/cr√©ation du sch√©ma '{schema}'")

        try:
            with engine.begin() as connection:
                connection.execute(text(f'CREATE SCHEMA IF NOT EXISTS "{schema}"'))
                logging.info(f"‚úÖ Sch√©ma '{schema}' v√©rifi√©/cr√©√© avec succ√®s.")

        except Exception as e:
            logging.error(f"‚ùå Erreur lors de la cr√©ation du sch√©ma '{schema}': {e}")
            raise AirflowFailException(f"Impossible de cr√©er le sch√©ma '{schema}': {e}")

    @staticmethod
    def __setup_table(
        engine: Engine,
        table_name: str,
        schema: str,
        columns: dict = None,
        primary_key: list = None
    ):
        """ Cr√©e une table dans la base de donn√©es s'il n'existe pas d√©j√†."""

        if not table_name or not table_name.strip():
            raise ValueError("Le nom de la table ne peut pas √™tre vide")

        if not re.match(r'^[a-zA-Z0-9_]+$', table_name):
            raise ValueError(f"Nom de table invalide: '{table_name}'")

        if not re.match(r'^[a-zA-Z0-9_]+$', schema):
            raise ValueError(f"Nom de sch√©ma invalide: '{schema}'")

        logging.info(f"‚è≥ V√©rification/cr√©ation de la table '{schema}.{table_name}'")

        try:
            # Construction des colonnes
            column_definitions = []
            if columns:
                for col, dtype in columns.items():
                    column_definitions.append(f'{col} {dtype}')

            # Ajout de la cl√© primaire si sp√©cifi√©e
            if primary_key and len(primary_key) > 0:
                primary_key_str = ', '.join(primary_key)
                column_definitions.append(f'PRIMARY KEY ({primary_key_str})')

            columns_sql = ', '.join(column_definitions)

            with engine.connect() as connection:
                connection.execute(text(f"""
                    CREATE TABLE IF NOT EXISTS {schema}.{table_name} (
                        {columns_sql}
                    )
                """))
                logging.info(f"‚úÖ Table '{schema}.{table_name}' v√©rifi√©e/cr√©√©e avec succ√®s.")

        except Exception as e:
            logging.error(f"‚ùå Erreur lors de la v√©rification de la table '{schema}.{table_name}': {e}")
            raise AirflowFailException(f"Impossible de v√©rifier la table: {e}") from e

    @staticmethod
    def __get_execution_date(**kwargs) -> pendulum.DateTime:
        """M√©thode priv√©e pour r√©cup√©rer le DataFrame et la date d'ex√©cution.

        Returns:
            pendulum.DateTime: Date d'ex√©cution
        """
        PARIS_TZ = "Europe/Paris"

        if 'logical_date' in kwargs:
            execution_date = kwargs['logical_date'].in_timezone(PARIS_TZ)
        elif 'execution_date' in kwargs:
            execution_date = kwargs['execution_date'].in_timezone(PARIS_TZ)
        elif 'ds' in kwargs:
            execution_date = pendulum.parse(kwargs['ds']).in_timezone(PARIS_TZ)
        else:
            raise KeyError("‚ùå Aucune date d'ex√©cution trouv√©e dans le contexte (logical_date, execution_date, ds)")

        logging.debug(f"üìÖ Date d'ex√©cution r√©cup√©r√©e : {execution_date}")
        return execution_date

    @customTask
    @staticmethod
    def extract(
        engine: Engine,
        table_name: str = None,
        schema: str = "public",
        schema_select: str = None,
        schema_where: str = None,
        schema_order: str = None,
        limit: int = None,
        skip_empty: bool = False,
        joins: list = None,
        **kwargs
    ) -> Any:
        """ Extrait des donn√©es d'un entrep√¥t de donn√©es (Data Warehouse).

        Args:
            engine (Engine): Moteur SQLAlchemy pour la connexion √† la base de donn√©es.
            table_name (str, optional): Nom de la table √† partir de laquelle extraire les donn√©es. Par d√©faut None.
            schema (str, optional): Sch√©ma de la base de donn√©es. Par d√©faut "public".
            schema_select (list, optional): Liste des colonnes √† s√©lectionner. Par d√©faut None (toutes les colonnes).
            schema_where (dict, optional): Dictionnaire des conditions WHERE pour filtrer les donn√©es. Par d√©faut None (aucun filtre).
            schema_order (str, optional): Colonne pour ordonner les r√©sultats. Par d√©faut None (pas d'ordre).
            limit (int, optional): Limite le nombre de lignes extraites. Par d√©faut None (aucune limite).
            skip_empty (bool, optional): Si True, saute la t√¢che si aucune donn√©e n'est extraite. Par d√©faut False.
            joins (list, optional): (avec alias t_main pour la table principale) Liste des jointures √† effectuer. Chaque √©l√©ment est un dictionnaire avec les cl√©s:
                - 'table': nom de la table √† joindre (format: schema.table ou table)
                - 'on': condition de jointure (ex: "t1.id = t2.id")
                - 'type': type de jointure (INNER, LEFT, RIGHT, FULL). Par d√©faut INNER.
                - 'alias': alias pour la table jointe (optionnel)

        Returns:
            Any: Donn√©es extraites sous forme de DataFrame ou autre format selon l'impl√©mentation.

        Examples:
            >>> PostgresWarehouse.extract(
            ...     engine=engine,
            ...     table_name="sales_data",
            ...     schema="public",
            ...     schema_select=["s.id", "s.amount", "c.name"],
            ...     joins=[
            ...         {
            ...             'table': 'customers',
            ...             'alias': 'c',
            ...             'on': 's.customer_id = c.id',
            ...             'type': 'INNER'
            ...         }
            ...     ],
            ...     schema_where={"s.year": 2023}
            ... )
            Extrait des donn√©es de sales_data avec une jointure INNER sur customers.
        """
        query = f"\nSELECT \n\t * \n\r FROM \n\t {schema}.{table_name} "

        # Gestion des alias pour la table principale
        main_table_alias = ""
        if joins:
            # Si on a des jointures, on ajoute un alias √† la table principale pour √©viter l'ambiguit√©
            main_table_alias = "t_main"
            query = f"\nSELECT \n\t * \n\r FROM \n\t {schema}.{table_name} AS {main_table_alias} "

        if schema_select:
            select_columns = '\n\t, '.join(schema_select)
            if joins:
                query = f"\nSELECT \n\t{select_columns} \nFROM \n\t{schema}.{table_name} AS {main_table_alias} "
            else:
                query = f"\nSELECT \n\t{select_columns} \nFROM \n\t{schema}.{table_name} "

        # Construction des jointures
        if joins:
            for join in joins:
                join_type = join.get('type', 'INNER').upper()
                join_table = join['table']
                join_on = join['on']
                join_alias = join.get('alias', '')
                
                # Ajouter le sch√©ma par d√©faut si pas sp√©cifi√© dans la table
                if '.' not in join_table:
                    join_table = f"{schema}.{join_table}"
                
                # Construction de la clause JOIN
                if join_alias:
                    query += f"\n{join_type} JOIN \n\t{join_table} AS {join_alias} \n\t\tON {join_on}"
                else:
                    query += f"\n{join_type} JOIN \n\t{join_table} \n\t\tON {join_on}"
        if schema_where:

            where_conditions = []
            params = {}
            for col, val in schema_where.items():
                # Si la valeur commence par "is" (is null, is not null, etc.), ne pas utiliser de param√®tre
                if isinstance(val, str) and val.lower().startswith('is '):
                    where_conditions.append(f"\n\t {col} {val}")
                else:
                    # Sinon, utiliser un param√®tre pour l'√©galit√©
                    where_conditions.append(f"\n\t {col} = :{col}")
                    params[col] = val

            if where_conditions:
                query += "\nWHERE " + "\n\tAND ".join(where_conditions)

        if schema_order:
            query += f"\nORDER BY \n\t{schema_order}"

        if limit:
            query += f"\nLIMIT {limit}"

        logging.info(f"‚è≥ Execution de la requ√™te: {query} ...")

        with engine.begin() as connection:

            result = connection.execute(text(query), **(schema_where or {}))
            df = pd.DataFrame(result.fetchall(), columns=result.keys())

        if df.empty:
            message = "‚ö†Ô∏è Aucune donn√©e extraite du Data Warehouse."

            if skip_empty:
                logging.warning(message + " La t√¢che sera saut√©e.")
                raise AirflowSkipException(message + " La t√¢che sera saut√©e.")

            logging.warning(message)

        logging.info(f"‚úÖ Requ√™te ex√©cut√©e avec succ√®s, {len(df)} lignes extraites.")

        return manager.Xcom.put(
            input=df,
            **kwargs
        )

    @customTask
    @staticmethod
    def extract_from_dict(
        xcom_source: str,
        key: str,
        **kwargs
    ) -> Any:
        """ Extrait un √©l√©ment sp√©cifique d'un dictionnaire stock√© dans XCom.

        Args:
            xcom_source (str): Identifiant de la t√¢che Airflow dont le dictionnaire sera extrait via XCom.
            key (str): Cl√© de l'√©l√©ment √† extraire du dictionnaire.
            **kwargs: Contexte d'ex√©cution Airflow.

        Returns:
            Any: L'√©l√©ment extrait du dictionnaire (g√©n√©ralement un DataFrame).

        Examples:
            >>> PostgresWarehouse.extract_from_dict(
            ...     xcom_source="fetch_task",
            ...     key="match_data"
            ... )
            Extrait l'√©l√©ment "match_data" du dictionnaire retourn√© par "fetch_task".
        """

        data = manager.Xcom.get(xcom_source=xcom_source, skip_if_empty=True, **kwargs)

        if not isinstance(data, dict):
            raise AirflowFailException(f"Les donn√©es r√©cup√©r√©es ne sont pas un dictionnaire. Type re√ßu: {type(data)}")

        if key not in data:
            raise AirflowFailException(f"La cl√© '{key}' n'existe pas dans le dictionnaire. Cl√©s disponibles: {list(data.keys())}")

        extracted_data = data[key]
        logging.info(f"‚úÖ √âl√©ment '{key}' extrait avec succ√®s du dictionnaire")
        logging.info(f"‚ÑπÔ∏è Nombre d'√©l√©ments extraits: {len(extracted_data) if hasattr(extracted_data, '__len__') else 'N/A'}")

        return manager.Xcom.put(
            input=extracted_data,
            **kwargs
        )

    @customTask
    @staticmethod
    def insert(
        xcom_source : str,
        engine : Engine,
        table_name: str = "test_table",
        schema: str = "public",
        chunksize: int = 1000,
        method : str = None,
        if_table_exists: str = "append",
        add_technical_columns: bool = False,
        **kwargs
    ) -> dict:
        """ Ins√®re des donn√©es dans un entrep√¥t de donn√©es (Data Warehouse).

        Args:
            xcom_source (str): Identifiant de la t√¢che Airflow dont les donn√©es seront extraites via XCom.
            engine (Engine): Moteur SQLAlchemy pour la connexion √† la base de donn√©es.
            table_name (str, optional): Nom de la table dans laquelle ins√©rer les donn√©es. Par d√©faut "test_table".
            schema (str, optional): Sch√©ma de la base de donn√©es. Par d√©faut "public".
            chunksize (int, optional): Nombre de lignes √† ins√©rer par lot. Par d√©faut 1000.
            method (str, optional): M√©thode d'insertion. Par d√©faut None.
            if_table_exists (str, optional): Comportement si la table existe d√©j√† ("fail", "replace", "append"). Par d√©faut "append".

        Returns:
            dict: Dictionnaire contenant le nom de la table, le sch√©ma et le nombre de lignes ins√©r√©es.

        Examples:
            >>> PostgresWarehouse.insert(
            ...     xcom_source="extract_task",
            ...     engine=engine,
            ...     table_name="sales_data",
            ...     schema="public",
            ...     chunksize=500,
            ...     method=None,
            ...     if_table_exists="append"
            ... )
            Ins√®re les donn√©es extraites par la t√¢che "extract_task" dans la table "public.sales_data" du Data Warehouse.
        """

        df = manager.Xcom.get(xcom_source=xcom_source, skip_if_empty=True, **kwargs)

        if not isinstance(df, pd.DataFrame):
            raise AirflowFailException(f"‚ùå Les donn√©es r√©cup√©r√©es depuis '{xcom_source}' ne sont pas un DataFrame. Type re√ßu: {type(df)}")

        if df.empty:
            raise AirflowFailException("‚ùå Aucune donn√©e √† ins√©rer dans le Data Warehouse (DataFrame vide).")

        if add_technical_columns:
            execution_date = PostgresWarehouse.__get_execution_date(**kwargs)
            df["tech_dag_id"] = kwargs['dag'].dag_id
            df["tech_execution_date"] = execution_date.strftime("%Y-%m-%d %H:%M:%S")

        PostgresWarehouse.__setup_schema(engine, schema)

        logging.info(f"‚è≥ Insertion de {len(df)} ligne(s) dans la table '{schema}.{table_name}'")

        try:
            df.to_sql(name=table_name, schema=schema ,con=engine, if_exists=if_table_exists, index=False, chunksize=chunksize, method=method,)

        except Exception as e:
            raise AirflowFailException(f"‚ùå Erreur lors de l'insertion dans le Data Warehouse: {e}") from e

        logging.info(f"‚úÖ Insertion termin√©e avec succ√®s dans la table '{schema}.{table_name}'")

        return {
            'table_name': table_name,
            'schema': schema,
            'rows_inserted': len(df),
        }

    @customTask
    @staticmethod
    def update(
        engine: Engine,
        table_name: str = "test_table",
        schema: str = "public",
        set_values: dict = None,
        where_conditions: dict = None,
        **kwargs
    ) -> dict:
        """ Met √† jour des donn√©es dans un entrep√¥t de donn√©es (Data Warehouse).

        Args:
            engine (Engine): Moteur SQLAlchemy pour la connexion √† la base de donn√©es.
            table_name (str, optional): Nom de la table √† mettre √† jour. Par d√©faut "test_table".
            schema (str, optional): Sch√©ma de la base de donn√©es. Par d√©faut "public".
            set_values (dict): Dictionnaire des colonnes et valeurs √† mettre √† jour.
            where_conditions (dict): Dictionnaire des conditions WHERE pour filtrer les lignes √† mettre √† jour.

        Returns:
            dict: Dictionnaire contenant le nom de la table, le sch√©ma et le nombre de lignes mises √† jour.

        Examples:
            >>> PostgresWarehouse.update(
            ...     engine=engine,
            ...     table_name="sales_data",
            ...     schema="public",
            ...     set_values={"amount": 200.0},
            ...     where_conditions={"id": 1}
            ... )
            Met √† jour la colonne "amount" √† 200.0 dans la table "publica.sales_data" pour l'enregistrement o√π l'id est 1.
        """

        if not set_values:
            raise ValueError("Aucune valeur sp√©cifi√©e pour la mise √† jour.")

        if not where_conditions:
            raise ValueError("Aucune condition WHERE sp√©cifi√©e pour la mise √† jour.")

        # Fonctions SQL natives qui ne doivent pas √™tre param√©tr√©es
        SQL_FUNCTIONS = ['CURRENT_TIMESTAMP', 'CURRENT_DATE', 'CURRENT_TIME', 'NOW()', 'NULL']

        # Construction de la clause SET en distinguant les fonctions SQL des valeurs
        set_parts = []
        parameters = {}

        for col, val in set_values.items():
            if isinstance(val, str) and val.upper() in SQL_FUNCTIONS:
                # Fonction SQL native - ins√©rer directement
                set_parts.append(f"{col} = {val}")
            else:
                # Valeur param√©trable
                set_parts.append(f"{col} = :set_{col}")
                parameters[f"set_{col}"] = val

        set_clause = ', '.join(set_parts)

        # Construction de la clause WHERE
        where_parts = []
        for col, val in where_conditions.items():
            # Extraire la valeur si c'est une Series pandas
            if hasattr(val, 'iloc'):
                val = val.iloc[0] if len(val) > 0 else val
            where_parts.append(f"{col} = :where_{col}")
            parameters[f"where_{col}"] = val

        where_clause = ' AND '.join(where_parts)

        update_query = f"""
            UPDATE {schema}.{table_name}
            SET {set_clause}
            WHERE {where_clause}
        """

        logging.info(f"‚è≥ Ex√©cution de la requ√™te de mise √† jour: {update_query}")
        logging.info(f"Param√®tres set : {set_values}")
        logging.info(f"Param√®tres where : {where_conditions}")

        try:
            with engine.begin() as connection:
                result = connection.execute(text(update_query), **parameters)
                logging.info(f"‚úÖ Mise √† jour termin√©e avec succ√®s dans la table '{schema}.{table_name}'. Lignes affect√©es: {result.rowcount}")

        except Exception as e:
            logging.error(f"‚ùå Erreur lors de la mise √† jour dans le Data Warehouse: {e}")
            raise

        return {
            'table_name': table_name,
            'schema': schema,
            'rows_updated': result.rowcount,
        }

    @customTask
    @staticmethod
    def insert_lines(
        engine: Engine,
        table_name: str = "test_table",
        schema: str = "public",
        lines: list = None,
        **kwargs
    ) -> dict:
        """ Ins√®re des lignes sp√©cifiques dans une table d'un entrep√¥t de donn√©es (Data Warehouse).

        Args:
            engine (Engine): Moteur SQLAlchemy pour la connexion √† la base de donn√©es.
            table_name (str, optional): Nom de la table dans laquelle ins√©rer les lignes. Par d√©faut "test_table".
            schema (str, optional): Sch√©ma de la base de donn√©es. Par d√©faut "public".
            lines (list, optional): Liste des dictionnaires repr√©sentant les lignes √† ins√©rer.

        Returns:
            dict: Dictionnaire contenant le nom de la table, le sch√©ma et le nombre de lignes ins√©r√©es.

        Examples:
            >>> PostgresWarehouse.insert_lines(
            ...     engine=engine,
            ...     table_name="sales_data",
            ...     schema="public",
            ...     lines=[
            ...         {"id": 1, "amount": 100.0, "date": "2023-01-01"},
            ...         {"id": 2, "amount": 150.0, "date": "2023-01-02"}
            ...     ]
            ... )
            Ins√®re deux lignes sp√©cifiques dans la table "public.sales_data" du Data Warehouse.
        """

        if not lines:
            logging.warning("‚ùå Aucune ligne √† ins√©rer")
            return

        PostgresWarehouse.__setup_schema(engine, schema)
        PostgresWarehouse.__setup_table(engine, table_name, schema)

        try:
            with engine.begin() as connection:

                for row in lines:
                    columns = ', '.join(row.keys())
                    values = ', '.join([f":{key}" for key in row.keys()])
                    insert_query = f"INSERT INTO {schema}.{table_name} ({columns}) VALUES ({values})"
                    connection.execute(text(insert_query), **row)

            logging.info(f"‚úÖ {len(lines)} ligne(s) ins√©r√©e(s) avec succ√®s dans la table '{schema}.{table_name}'")

        except Exception as e:
            logging.error(f"‚ùå Erreur lors de l'insertion des lignes dans le Data Warehouse: {e}")
            raise

        return {
            'table_name': table_name,
            'schema': schema,
            'rows_inserted': len(lines),
        }

    @customTask
    @staticmethod
    def create_table(
        engine: Engine,
        table_name: str = "test_table",
        schema: str = "public",
        columns: dict = None,
        primary_key: list = None,
        **kwargs
    ) -> dict:
        """ Cr√©e une table fact dans un entrep√¥t de donn√©es (Data Warehouse).

        Args:
            engine (Engine): Moteur SQLAlchemy pour la connexion √† la base de donn√©es.
            table_name (str, optional): Nom de la table √† cr√©er. Par d√©faut "test_table".
            schema (str, optional): Sch√©ma de la base de donn√©es. Par d√©faut "public".
            columns (dict, optional): Dictionnaire des colonnes et leurs types de donn√©es.
            primary_key (list, optional): Liste des colonnes constituant la cl√© primaire.

        Returns:
            None
        """

        PostgresWarehouse.__setup_schema(engine, schema)
        PostgresWarehouse.__setup_table(engine, table_name, schema, columns, primary_key)

        logging.info(f"‚úÖ Table '{schema}.{table_name}' pr√™te dans le Data Warehouse.")

        return {
            'table_name': table_name,
            'schema': schema,
            'columns': columns or {},
        }

    @customTask
    @staticmethod
    def raw_to_fact(
        engine: Engine,
        outlets: list = None,
        source_table: str = None,
        target_table: str = None,
        join_keys: list = None,
        match_columns: dict = None,
        non_match_columns: dict = None,
        has_matched: bool = False,
        has_not_matched: bool = False,
        **kwargs
    ) -> dict:
        """ Transforme des donn√©es brutes en donn√©es factuelles dans un entrep√¥t de donn√©es (Data Warehouse).

        Args:
            engine (Engine): Moteur SQLAlchemy pour la connexion √† la base de donn√©es.
            outlets (list, optional): Liste des assets de sortie pour le suivi des donn√©es.
            source_table (str): Nom de la table source contenant les donn√©es brutes.
            target_table (str): Nom de la table cible pour les donn√©es factuelles.
            schema (str, optional): Sch√©ma de la base de donn√©es. Par d√©faut "public
            join_keys (list, optional): Liste des colonnes utilis√©es pour faire le lien entre les tables source et cible.
            match_columns (dict, optional): Dictionnaire des colonnes √† mettre √† jour en cas de correspondance.
            non_match_columns (dict, optional): Dictionnaire des colonnes √† ins√©rer en cas de non-correspondance.
            has_matched (bool, optional): Indique si les lignes correspondantes doivent √™tre mises √† jour. Par d√©faut False.
            has_not_matched (bool, optional): Indique si les lignes non correspondantes doivent √™tre ins√©r√©es. Par d√©faut False.

        Returns:
            None
        """

        if has_matched is False and has_not_matched is False:
            raise ValueError("Aucune op√©ration de transformation sp√©cifi√©e (has_matched et has_not_matched sont tous deux False). Aucune action effectu√©e.")

        transformation_result = {
            'source_table': source_table,
            'target_table': target_table,
            'operations_performed': [],
            'rows_affected': 0,
            'timestamp': pd.Timestamp.now().isoformat(),
            'outlets_materialized': outlets or [],
        }

        try:
            with engine.begin() as connection:

                matched = ""
                not_matched = ""

                if has_matched:
                    matched = f"""
                    WHEN MATCHED THEN
                        UPDATE SET
                            {', '.join([f'{col} = raw.{col}' for col in match_columns.keys()])},
                            tech_date_modification = CURRENT_TIMESTAMP
                    """
                    transformation_result['operations_performed'].append('UPDATE')

                if has_not_matched:
                    not_matched = f"""
                    WHEN NOT MATCHED THEN
                        INSERT ({', '.join(non_match_columns.keys())}, tech_date_creation, tech_date_modification)
                        VALUES ({', '.join([f'raw.{col}' for col in non_match_columns.values()])}, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)
                    """
                    transformation_result['operations_performed'].append('INSERT')

                transform_query = f"""
                    MERGE INTO {target_table} AS fact
                    USING {source_table} AS raw
                    ON {" AND ".join([f"fact.{key} = raw.{key}" for key in join_keys])}

                    {matched}
                    {not_matched};
                """

                logging.info(f"‚è≥ Transformation des donn√©es brutes en donn√©es factuelles dans la table '{target_table}_fact'")
                logging.info(f"Requ√™te de transformation: {transform_query}")

                result = connection.execute(text(transform_query))
                transformation_result['rows_affected'] = result.rowcount

                logging.info(f"‚úÖ Transformation termin√©e avec succ√®s dans la table '{target_table}_fact'. Lignes affect√©es: {result.rowcount}")

        except Exception as e:
            logging.error(f"‚ùå Erreur lors de la transformation dans le Data Warehouse: {e}")
            transformation_result['error'] = str(e)
            raise

        return transformation_result